# 扩展型博弈

## 简介

在本节中，我们将探讨扩展型博弈。扩展型博弈是一种顺序博弈，其中包含一组参与者、规定参与者何时可以行动的规则、他们观察到的信息以及行动时获得的奖励。在扩展型博弈中，多个参与者可以在游戏中采取行动，但不会同时进行。游戏结束时，每位参与者都会获得一定的奖励，称为收益，可以是正值也可以是负值。

我们主要考察解决扩展型博弈的三种方法：

在基于模型的博弈中，我们可以使用逆向归纳法，即为博弈的每个子博弈计算均衡，并利用这些结果来决定我们的行动。

在无模型博弈中，我们可以使用无模型强化学习。这些技术与Q学习或策略梯度方法非常相似，区别在于有其他参与者会影响我们的奖励，而不仅仅是我们自身和环境。

如果我们有模拟环境，我们可以使用无模型技术或多智能体蒙特卡洛树搜索（MCTS），这与单智能体MCTS类似，不同之处在于同样有其他参与者会影响我们的奖励，而不仅仅是我们自身和环境。

在这些笔记中，我们仅关注完美信息扩展型博弈，这意味着博弈状态对所有参与者完全可观察。

## 完美信息扩展型博弈

> 定义 – 完美信息扩展型博弈

完美信息扩展型博弈是一个元组 $G = (N, S, s_0, A, T, r)$

1. $N$ 是包含 $n$ 个参与者的集合
2. $S$ 是状态（或节点）的集合
3. $s_0$ 是初始状态
4. $A: S \rightarrow 2^A$ 是一个函数，指定从每个状态 $s \in S$ 允许的行动
5. $P: S \rightarrow N$ 是一个函数，指定在节点选择行动的参与者（轮到谁行动）
6. $T : S \times A \rightarrow S$ 是转移函数，指定在某状态选择某行动后的后继状态
7. $r : S \rightarrow \mathbb{R}^N$ 是奖励函数，返回一个 $N$ 元组，指定每个参与者在状态 $S$ 中获得的收益。在某些文献中，这可能被称为 $u$ 而非 $r$，表示效用，但在这些笔记中我们将使用 $r$。

## 扩展型博弈的解法

与常规型博弈类似，扩展型博弈也有策略，但这些策略必须告诉每个参与者在轮到自己选择行动时该做什么。

> 定义 – 纯策略: 对于扩展型博弈 $G$，参与者 $i$ 的纯策略是笛卡尔积 $\Pi_{s\in S, P(s)=i}A(s)$。

因此，参与者 $i$ 的纯策略告诉他们在每个轮到自己行动的状态中应采取什么行动。扩展型博弈的最优解称为该博弈的子博弈完美均衡。在定义子博弈完美均衡之前，我们首先需要定义什么是子博弈。

> 定义 – 子博弈: 给定一个扩展型博弈 $G$，以节点 $s_g \in S$ 为根的子博弈是指博弈 $G_{s_g}= (N, S, s_{g}, A, T, r)$；即博弈树中以 $s_g$ 为根节点，且其后代与 $G$ 中的后代相同的部分。

对于博弈树中出现的任何状态 $s_g \in S$，我们可以通过取该树的后代来定义子博弈。因此，我们可以将一个博弈简单地定义为根节点 $s_0$，其中 $A(s_0)$ 中的行动通向的节点是其子博弈的根节点。

> 定义 – 子博弈完美均衡: 博弈 $G$ 的子博弈完美均衡（SPE）由 N 个参与者的所有策略组合构成，使得对于 $G$ 的任何子博弈 $G_{s_g}$，参与者 $P(s_g)$ 的策略在 $s_g$ 处是该参与者的最佳对策。

因此，子博弈完美均衡是每个参与者在轮到自己行动时的最佳对策。

## 逆向归纳法

逆向归纳法是解决扩展型博弈的一种基于模型的技术。它通过递归计算每个子博弈的均衡，然后利用这些结果来解决每个子博弈的父节点。由于它首先解决子博弈，因此实际上是从后向前解决整个博弈。

其直觉如下：从博弈树的终端节点（即那些 $A(s)$ 为空的节点）开始，对于父节点，计算轮到行动的参与者的最佳行动。这为博弈中最小的子博弈提供了子博弈均衡。由于这些解本身是奖励元组，我们可以通过将父节点的解作为子博弈的奖励来解决父节点的父节点，这为从终端节点的父节点的父节点开始的博弈提供了子博弈均衡。我们逐步将这些值向上归纳到树的起始节点。

在我们这里讨论的纯逆向归纳法中，假设只有终端节点具有奖励。如果我们想要建模非终端节点也有奖励的情况，只需将到达终端节点路径上的所有奖励相加即可。因此，这种解法可以推广到前一节中扩展型博弈的定义。

在以下算法中，$best_child$ 是一个 N 元组，用于找出轮到谁行动时的最佳子节点值；而 $best_child(P(s))$ 和 $child_reward(P(s))$ 从 $best_child$ 和 $child_reward$ 元组中返回从状态 $s$ 选择行动的参与者的值。

$$\begin{array}{l} Input:\ \text{Extensive form game}\ G = (N, Agt, S, s_0, A, T, r)\\ Output:\ \text{Sub-game equilibrium for each state}\ s \in S\\[2mm] Return\ BackwardInduction(s_0)\\[2mm] function\ BackwardInduction(s \in S) \\ \quad\quad if\ A(s) = \emptyset\ then \\ \quad\quad\quad\quad return\ r(s) \\ \quad\quad best\_child \leftarrow (-\infty, \ldots, -\infty) \\ \quad\quad foreach\ a \in A(s) \\ \quad\quad\quad\quad s' \leftarrow T(s, a) \\ \quad\quad\quad\quad child\_reward \leftarrow BackwardInduction(s') \\ \quad\quad\quad\quad if\ child\_reward(P(s)) > best\_child(P(s))\ then \\ \quad\quad\quad\quad\quad\quad best\_child \leftarrow child\_reward \\ \quad\quad return\ best\_child \end{array}$$

以上解法是一个递归算法，它返回终端节点的奖励元组，否则寻找节点子节点的最佳奖励元组。然而，"最佳"是相对于轮到谁行动而言的。理性参与者在轮到自己行动时会选择对自己最有利的结果。算法的最终输出是轮到谁行动时终端状态的奖励，这就是整个博弈的子博弈完美均衡。

该算法可以通过在每个点收集信息的方式进行简单修改，以返回每个参与者的路径和策略。
