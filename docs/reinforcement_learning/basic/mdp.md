# Markov Decision Process

## 简介

马尔可夫决策过程（Markov Decision Process，简称MDP）是一种描述序列决策问题的框架。在机器学习中，分类和回归等问题属于一次性任务，即分类/回归模型接收输入并返回输出，而下一个输入与前一个完全独立。而在序列决策问题中，我们需要随时间推移做出一系列决策，每个决策都会影响未来的可能性。例如，从一个地点导航到另一个地点需要我们选择方向和速度，按该方向以该速度移动，然后反复做出这个决策直到到达目的地。因此，即使在第一步，我们也必须考虑每个移动如何影响未来。另一个例子是，临床医生在制定医疗治疗决策时必须考虑今天所做的决策是否会影响未来对患者的治疗方案。

在强化学习的术语中，我们将决策者称为智能体（agent），而他们的决策则是在环境或状态中执行的动作（actions）。

启发式搜索和经典规划算法等技术假设动作是确定性的——即智能体在执行动作之前，已知该动作将产生的确切结果。而MDP则去除了确定性事件的假设，转而假设每个动作可能有多种结果，每种结果都与一个概率相关联。如果每个动作只有一个结果（概率为1），那么问题是确定性的；否则，它是非确定性的。MDP考虑的是随机性非确定性，即结果服从某种概率分布。

MDP已成功应用于多个领域的规划问题：机器人导航、矿区开采区域规划、患者治疗方案制定、车辆维护调度，以及许多其他领域。

## Markov Decision Processes

马尔可夫决策过程（MDP）是一种完全可观察的概率状态模型。最常见的MDP表述是折扣奖励马尔可夫决策过程。折扣奖励MDP是一个包含以下元素的元组：

* 状态空间 $S$；
* 初始状态 $s_0$；
* 智能体可以在每个状态 $s$ 中执行的动作集合 $A(s)$；
* 状态转移概率 $P(s'|s, a)$，表示在状态 $s$ 执行动作 $a$ 后转移到状态 $s'$ 的概率；
* 奖励函数 $R(s, a, s')$，表示从状态 $s$ 通过执行动作 $a$ 转移到状态 $s'$ 所获得的正面或负面奖励；以及
* 折扣因子 $\gamma$。

状态是智能体可能处于的各种情况。每个状态都捕获了做出决策所需的信息。例如，在机器人导航中，状态包括机器人的位置、当前速度、行进方向以及障碍物、门等的位置。在为快递公司安排车辆维护的应用中，状态则包括车辆ID、车辆属性（如品牌、最大载重量等）、车辆位置、自上次维护检查以来行驶的公里数等。

状态空间简单来说就是所有可能状态的集合。也就是所有车辆ID、车辆属性、最大载重量等的组合。

动作允许智能体影响环境/状态——即动作使环境从一个状态转变为另一个状态。它们也是智能体在每个状态下可用的选择：我现在应该选择哪个动作？目前，我们假设只有智能体可以影响状态。

如前所述，一个动作可能有多种可能的结果。确切地说，只会发生一种结果，但智能体在执行动作之前并不知道会是哪一种。

转移概率告诉我们每个动作的效果，包括每种结果的概率。例如，在车辆维护任务中，当我们的智能体安排车辆进行检查时，可能的结果包括：(a) 不需要进一步维护（80%的概率）；(b) 需要小型维护（15%的概率）；或(c) 需要大型维护（5%的概率）。

奖励明确了在特定状态下执行特定动作的收益或成本。例如，导航到目的地的机器人在到达目的地时会获得正面奖励（收益），在途中撞到物体时会得到小的负面奖励（成本），而撞到人时则会得到大的负面奖励。

折扣因子 $\gamma$ 决定了未来奖励相比当前奖励应该被折扣多少。

例如，你是更喜欢今天获得100元还是一年后获得100元？我们（人类）通常会对未来进行折扣，并对更近期的奖励赋予更高的价值。

假设我们的智能体按顺序收到奖励 $r_1, r_2, r_3, ...$。如果 $\gamma$ 是折扣因子，那么折扣奖励为：

$$V = r_1 + \gamma r_2 + \gamma^{2}r_3 + \gamma^{3}r_4 + \cdots \\ = r1 + \gamma(r_2 + \gamma(r_3 + \gamma(r_4 + \cdots)))$$

如果 $V_t$ 是在时间步 $t$ 收到的奖励值，那么 $V_t = r_t + \gamma V_{t+1}$。因此，奖励离初始状态 $s_0$ 越远，我们从中获得的实际奖励就越少。

在MDP中，折扣因子必须严格小于1。稍后，我们将了解为什么这一点很重要。
