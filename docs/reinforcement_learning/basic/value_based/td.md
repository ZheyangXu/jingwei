# Temporal difference reinforcement learning

## Model-based vs model-free

价值迭代是一类被称为基于模型的技术的解决方案。这意味着我们需要知道模型；特别是，我们可以访问 $P(s'|s, a)$ 和 $R(s, a, s')$。

在本节中，我们将研究 Q-learning 和 SARSA，它们是无模型技术。这意味着我们不知道模型的 $P(s'|s, a)$ 和 $R(s, a, s')$。

如果我们不知道转移概率和奖励，我们如何计算策略呢？我们通过尝试动作并观察结果来从经验中学习，这使其成为一个机器学习问题。

重要的是，在无模型强化学习中，我们不尝试学习 $P(s'|s, a)$ 或 $R(s, a, s')$ — 我们直接学习价值函数或策略。

在基于模型和无模型之间还有一种方法：基于模拟的技术。在这些情况下，我们有一个作为模拟器的模型，所以我们可以模拟 $P(s'|s, a)$ 和 $R(s, a, s')$ 并用无模型技术学习策略，但我们不能"看到" $P(s'|s, a)$ 和 $R(s, a, s')$，所以像价值迭代这样的基于模型的技术是不可能的。

## Model-free Reinforcement Learning

无模型强化学习有许多不同的技术，但它们都基于相同的基本原理。

* 我们执行我们想要解决的问题的许多不同回合（episodes），以学习一个策略。
* 在每个回合中，我们在执行动作和学习策略之间循环。
* 当我们的智能体执行一个动作时，我们会获得一个奖励（可能为0），并且我们可以看到执行该动作后产生的新状态。
* 基于此，我们强化我们对在先前状态下应用先前动作的估计。
* 然后，我们选择一个新的动作并在环境中执行它。
* 我们重复这个过程，直到以下情况之一发生：(1) 训练时间用完；(2) 我们认为我们的策略已经收敛到最优策略；或 (3) 我们的策略"足够好"（对于每个新回合，我们看到的改进微乎其微）。在实践中，很难知道我们是否已经收敛到最优策略。

## Monte-Carlo Reinforcement Learning

Monte-Carlo 强化学习可能是最简单的强化学习方法，它基于动物如何从环境中学习。其直觉相当简单。维护一个 Q 函数，记录每个状态-动作对的值 $Q(s, a)$。在每一步：(1) 使用多臂老虎机算法选择一个动作；(2) 应用该动作并获得奖励；(3) 基于该奖励更新 $Q(s, a)$。在多个回合中重复，直到... 什么时候？

它被称为 Monte-Carlo 强化学习，是以摩纳哥（法国里维埃拉的一个小公国）内的蒙特卡洛地区命名的，该地区以其奢华的赌场而闻名。由于赌博和赌场在很大程度上与机会相关，使用一些随机性来探索动作的模拟通常被称为蒙特卡洛方法。

算法 3 (Monte-Carlo 强化学习)

$$\begin{array}{l} \text{初始化}\ Q(s, a) \text{为任意值}\\ \text{初始化}\ N(s, a) = 0\\ \text{对所有}\ s \in S, \ a \in A(s)\ \text{重复（对每个回合）:}\\ \quad \text{初始化}\ S_0\\ \quad \pi \leftarrow \text{从}\ Q\ \text{派生的策略（例如，}\epsilon\text{-贪心）}\\ \quad \text{按照}\ \pi\ \text{生成一个回合：}\ S_0, A_0, R_1, S_1, A_1, R_2, ..., S_{T-1}, A_{T-1}, R_T, S_T\\ \quad G \leftarrow 0\\ \quad \text{对于}\ t = T-1, T-2, ..., 0\ \text{执行:}\\ \quad\quad G \leftarrow \gamma G + R_{t+1}\\ \quad\quad \text{如果这是第一次访问}\ (S_t, A_t)\ \text{在回合中:}\\ \quad\quad\quad N(S_t, A_t) \leftarrow N(S_t, A_t) + 1\\ \quad\quad\quad Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \frac{1}{N(S_t, A_t)}[G - Q(S_t, A_t)]\ \end{array}$$

该算法按照某种策略（如 $\epsilon$-贪心）生成整个回合，观察每一步 $t$ 的奖励 $R_t$。然后计算每一步的折扣未来奖励 $G$。如果 $(S_t, A_t)$ 在回合中较早出现，那么我们不更新 $Q(S_t, A_t)$，因为我们将在循环的后面更新它。如果它没有出现，我们将 $Q(S_t, A_t)$ 更新为所有回合中执行 $(S_t, A_t)$ 的所有执行的累积平均值。在此算法中，$N(S_t, A_t)$ 表示在所有回合中评估 $(S_t, A_t)$ 的次数。

## Q-Table

Q-table 是维护 Q 函数的最简单方式。它们是一个表格，为每个 $(s, a)$ 对都有一个条目。因此，与价值迭代中的价值函数一样，它们不能扩展到大型状态空间。（关于扩展的更多内容将在下一讲中介绍）。

最初，我们会有一个任意的 Q-table，如果所有值都初始化为零，在 GridWorld 示例中可能看起来像这样：

|状态/动作|	上|	右|	下|	左|
|---|---|---|---|---|
|(0, 0)|	0|	0|	0|	0|
|(0, 1)|	0|	0|	0|	0|
|(0, 2)|	0|	0|	0|	0|
|(1, 0)|	0|	0|	0|	0|
|(1, 1)|	0|	0|	0|	0|
|(1, 2)|	0|	0|	0|	0|
|(2, 0)|	0|	0|	0|	0|
|(2, 1)|	0|	0|	0|	0|
|(2, 2)|	0|	0|	0|	0|
经过一些训练后，我们可能会得到一个看起来像这样的 Q 函数：

|状态/动作|	上|	右|	下|	左|
|---|---|---|---|---|
|(0, 0)|	0.4|	0.5|	0.3|	0.2|
|(0, 1)|	0.3|	0.6|	0.4|	0.5|
|(0, 2)|	0.2|	0.1|	0.5|	0.7|
|(1, 0)|	0.5|	0.7|	0.2|	0.1|
|(1, 1)|	0.6|	0.8|	0.4|	0.6|
|(1, 2)|	0.3|	0.2|	0.7|	0.9|
|(2, 0)|	0.6|	0.1|	0.1|	0.2|
|(2, 1)|	0.8|	0.2|	0.2|	0.3|
|(2, 2)|	0.0|	0.0|	0.0|	0.0|

在强化学习过程中，随着智能体探索环境并更新 Q 值，这个表格会逐渐填充有意义的值，反映每个状态-动作对的预期未来奖励。例如，在这个例子中，我们可以看到如果目标位于 (2, 2)（可能是终止状态，所以所有动作的 Q 值都为 0），随着接近目标，导向目标的动作的 Q 值变得更高。从 (1, 2) 向左移动和从 (2, 1) 向上移动的较高值表明这些动作是通向目标的最佳路径。
