# Modeling and Abstraction for MDPs

## 简介

强化学习算法常常面临规模扩展的挑战。也就是说，随着问题的状态空间和动作空间变得更大，获得良好策略所需的时间呈指数级增长。

如果条件允许，有两种实用策略可以应用：$1$ 为问题投入更多的计算资源，以便运行更多回合或评估更多状态；或 $2$ 提高模拟器的效率（如果您正在使用基于模拟的技术）。

然而，我们可能已经用尽了所有可用的计算资源，或者我们根本不是在模拟环境中工作。在这些情况下，一种方法是简化问题并解决简化后的问题。

在本节中，我将提供一些在应用强化学习技术时我发现有用的技巧。实施这些技巧需要我们理解所处的领域。这样的实施只会在该领域或其他非常相似的领域有效——它们不太可能泛化到其他领域。然而，这些技巧/策略本身是通用的，如果我们花时间应用它们，它们可以在许多领域中发挥作用。

## 抽象化

一种解决方案是定义 MDP 的抽象版本，解决该抽象问题，然后将策略转换回原始问题。使用抽象，我们采用已有的原始 MDP 模型 $M$，并定义一个比原始模型更小的新 MDP 模型 $M_A$：它具有比原始模型更少的状态和/或更少的动作。

这个过程中，建模者应该问自己以下问题：

1. 在这个问题中，为了达到某个目标，我需要做出哪些重要决策？
2. 我需要什么信息来做到这一点？
3. 我能否将原始 MDP 的动作和状态空间转换为更小的动作和状态空间，以便获取这些信息并丢弃一些不太重要的信息？

然后，我们定义一个抽象模型 $M' = (S', s'_0, A', P', r', \gamma')$，它是 $M$ 的抽象，使得存在某种映射 $f_s : S \rightarrow S'$，使得 $f_s(s)$ 将状态 $s$ 抽象为另一个状态 $s'$；同样，对于 $A$、$P$、$r'$，甚至 $\gamma$ 也有类似的映射（如果动作空间更小，计划的长度会不同）。

我们解决新模型 $M'$，获得抽象策略 $\pi' : S' \rightarrow A'$。然后，我们需要定义一个映射 $g : A' \rightarrow A$ 将每个抽象动作映射到原始模型中的动作。有了这些，我们可以为 $M$ 定义策略如下：

$$\pi(s) = g(\pi'(f_s(s)))$$

这意味着 $\pi(s)$ 使用 $f_s$ 将 $s$ 转换为 $s' \in S'$，然后应用抽象策略 $\pi'$ 获得抽象动作 $a' \in A'$，最后将其映射到具体动作 $a \in A$。

这里的"模型"不仅仅指基于模型的强化学习：即使在无模型环境中，我们也可以通过在环境和强化学习算法之间放置一个"层"来抽象我们看到的动作和状态，并修改奖励。

例如，在这些笔记中，我们使用网格世界（GridWorld）问题来说明各种技术，因为它简单直观。这已经是对真实导航问题的抽象。在真实的导航问题中，机器人必须在连续环境中移动，执行旋转和加速等动作。然而，我们在问题中的目标是找到最佳路线，而不是移动车轮。为了找到最佳路线，我们通过将状态空间划分为网格并将动作映射为方向来以不同方式建模问题，因为这是我们找到一条良好（但可能不是最优）路线所需的全部信息。如果我们真的想移动机器人，我们需要一个中间层：(1) 将真实状态转换为我们抽象的网格坐标，并识别我们何时处于奖励状态，这是函数 $f_s$；以及 (2) 将我们的抽象动作转换回环境中的动作，这是函数 $g$。例如，动作"右"可能转换为旋转90度，然后前进2米。

有时，问题可能已经达到了尽可能抽象的程度。例如，如果我们正在学习制造设施中机械臂的控制器，来自传感器和致动器的信息集可能会强烈影响高级动作的效果，因此不能被抽象掉。然而，在实践中，通常有一些我们可以使用的抽象。

实际上，本节中的大多数技术都是某种形式的抽象技术，但一些其他技巧也能有所帮助。

## 状态抽象

在某些情况下，状态空间的大小可以通过组合一些不同的状态来抽象，这些状态要么从语义角度看是等价的，要么足够相似，大多数时候会导致相同的动作。

著名的围棋代理 AlphaGoZero 使用状态抽象，注意到游戏状态是对称的：它是一个 19x19 的网格，如果第一步是在某个角落放置一个棋子，那么具体是哪个角落并不重要：在所有四个角落放置棋子对应于相同的抽象状态。这将状态空间的大小减少到原来的四分之一，并且不会丢失任何信息。

其他抽象技术可能会丢失信息，但仍然有用。例如，GridWorld 中从精确坐标到简单网格的状态抽象。

## 动作抽象

类似于状态抽象，我们可以抽象动作。主要有两种方式：

1. 将较小动作的序列抽象为单个更高级别的动作，例如在 GridWorld 领域中，我们将物理导航动作抽象为仅仅是左、右、上和下。
2. 我们还可以将类似的动作分组为单个动作。例如，在 GridWorld 领域中，如果我们有像旋转这样的低级动作，它按给定的度数旋转，而不是考虑 360 种不同版本的旋转（每个度数一个），我们可以将动作分组到桶中：一个抽象动作用于转动 0-15 度，一个用于 16-30 度，等等。我们会失去信息/精度，但在许多情况下，解决方案可能已经足够好，并且问题更容易解决。

## 奖励子目标

在某些问题中，存在需要实现的最终目标，当我们转换到实现这些目标的状态时会获得奖励。我们在这些笔记中看到的一个问题，例如在 Freeway 示例中，是当这些奖励与起始状态相距甚远时。由于我们通常从随机策略或价值/Q 函数开始，在通过实现目标获得奖励之前，我们会花费大量时间进行随机模拟。

然而，这些问题通常有需要在途中实现的子目标。如果我们不实现子目标，就无法实现主要目标。例如，在 Freeway 游戏中，我们需要到达第 1 行，然后是第 2 行，等等，直到到达另一侧。

如果我们能够识别这些子目标，我们可以在通往目标的途中为子目标提供"部分"奖励。在这种情况下，我们识别关键状态（或更准确地说，关键转换），它们就像我们问题的子目标。在 Freeway 中，假设到达另一侧的奖励是 100 分，我们可以为到达第 1 行给予 1 分，为到达第 2 行给予 2 分，等等，然后为到达另一侧给予 100 - (1 + 2 + … + n) 分，其中 $n$ 是倒数第二行。

对于 MDP $(S, s_0, A, P, r, \gamma)$，流程是：

1. 识别需要实现以达成最终目标的子目标。
2. 创建一个新的奖励函数 $r'$，其中奖励被重新分配给子目标，并且使用原始奖励 $r$ 的任何到达真实目标的情节 $e$ 的累积奖励大致等于使用 $r'$ 的 $e$ 的累积奖励。
3. 解决新的 MDP $(S, s_0, A, P, r', \gamma)$ 以获得策略 $\pi$（MDP 相同，但具有不同的奖励函数）。
4. 在原始 MDP 上评估 $\pi$。

例如，考虑单人纸牌游戏纸牌接龙。游戏的目标是通过将所有卡牌放入四堆赢得游戏，移动卡牌到这些堆时有各种规则。每堆必须按照从 A 到 K 的顺序排列，且都是同一花色。

想象我们获得了纸牌接龙的模拟器，并且只有在通过到达卡牌正确放置在堆中的终端状态之一赢得游戏时才获得奖励。这个奖励只会在终端状态下给予，可能需要几百个动作才能到达。

这样的游戏模拟必须很长，因为直到游戏结束才会有奖励，而且在第一次赢得游戏之前，我们会花费大量随机动作。通过重新分配子目标，每当正确的卡牌放置在堆上时都给予奖励，我们可以更早地提供奖励，并为我们的强化学习算法提供可利用的信息。

这个想法可以通过奖励塑形来实现。在这些笔记中，我们专注于基于势能的奖励塑形，它提供小的"虚假"奖励。这实际上要求您为状态定义一个启发式方法，在学习初期信息不足时引导代理朝着有前途的动作前进。通过使用子目标，我们可以给任何作为子目标的状态提供一个塑形奖励 $\Phi(s)$，并为任何不是子目标的状态分配 $\Phi(s)=0$。将奖励塑形视为子目标通常在概念上比将其视为启发式方法更简单实现。

或者，我们可以用新的奖励函数"拦截"真实奖励函数，而不使用奖励塑形，但奖励塑形是一种更有原则的方法，并且在使用 Q 表和线性近似时提供收敛保证。

## 部分可观察状态变量

我们经常遇到某些变量在游戏的至少部分时间内不可观察的情况。例如，在纸牌接龙中，抽牌堆中的卡牌在我们取出每一张之前是不可见的。在这种情况下，模型是一个部分可观察马尔可夫决策过程（POMDP）。我们可以将其转换为 MDP，其中状态不表示为现实世界中的状态，而是对现实世界中状态的信念。然而，这种计算复杂度很高。

一种选择是通过忽略部分可观察变量来简化。在纸牌接龙中，只考虑我们已经可以看到的卡牌的动作而不跟踪牌堆中的卡牌是足够简单的。在纸牌接龙中，这将很有效。如果你曾经玩过这个游戏，你会知道你花很少的时间考虑牌堆中的卡牌。

当然，这并不总是一个好主意：有时跟踪部分可观察变量很重要；例如，通过映射某些卡牌在未来可能被打出的概率。

## 启发式方法

使用启发式方法是加速学习的好方法。我们通常可以在三个地方使用启发式方法。

状态/动作剪枝：首先，如果我们有一个合理的启发式方法，我们可以选择"剪枝"不太有希望的状态，只探索那些看起来有希望的状态。例如，考虑对纸牌接龙使用 MCTS。如果我们使用模型或模拟器，我们可能能够窥视动作产生的状态。如果一个动作使我们能够将一组卡牌移动到目标堆中，我们可能会优先使用该动作进行模拟。鉴于我们在扩展任何其他节点之前完全扩展第一个节点，我们可以查看每个节点的启发式值，如果它太低，就将其从树中剪除。这将减少我们的搜索空间。当然，一个糟糕的启发式方法可能会造成更多的损害！但不仅仅是 MCTS 受益。例如，使用 Q-learning 或 SARSA，当我们到达不太有希望的状态时，我们可以终止情节，要么开始新的情节，要么如果使用模拟器，则回溯到较早的有希望节点。

利用：其次，启发式方法有助于引导情节走向好的解决方案。在任何无模型或基于模拟的技术中，早期情节实际上以均匀概率选择每个动作，因为没有好的信息可以利用。与奖励塑形一样，我们可以使用启发式方法更频繁地选择最有希望的动作，这意味着模拟仍然是随机的，但是有信息指导。在 MCTS 中，当我们扩展一个节点然后进行模拟时，这特别有价值，因为如果我们能找到合理的模拟，我们会更好地了解扩展节点的价值。这里的技巧是平衡启发式方法与我们学习到的 Q 函数中的信息。我们可以使用加权度量，或者在 Q 函数中获得足够信息后停止使用启发式方法。

更短的情节和模拟：最后，我们可以使用启发式方法提前终止情节/模拟。当我们朝着终端状态移动时，我们的启发式值可能更准确，因为随着我们获得更多信息，"更容易"估计状态的好坏。因此，我们可以使用启发式方法提前终止情节。例如，在 MCTS 中，我们可以模拟 $X$ 个动作，然后返回状态的启发式值，而不是模拟到终端状态。这是一个估计，但在学习过程的早期，运行返回启发式值的较短模拟通常比运行耗时更长且通常涉及许多无用动作的更长模拟更有价值。实际上，这正是像 Q-learning 和 SARSA 这样的时间差分方法所做的！它们使用 $\max_a Q(s', a')$ 和 $Q(s', a')$ 作为状态 $s'$ 值的启发式估计，以便我们可以为先前执行的动作分配"信用"。在这些技术中，我们正在学习启发式方法。手工制作的启发式方法在早期情节中会更有帮助。

## 要点

1. 解决 MDP 的技术面临可扩展性问题。
2. 使用建模技巧，我们可以找到更容易解决的问题，并将其应用回原始问题。
3. 有时更小的问题足以解决我们的问题；其他时候则不然。
